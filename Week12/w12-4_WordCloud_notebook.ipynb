{"cells":[{"cell_type":"markdown","metadata":{"id":"WW3GvR-Qxfpb"},"source":["# 文書中の単語をカウントして,WordCloudに出力する\n","\n","準備：<br>\n","- 表示のためのフォント「IPAGTTC00303」（解凍済み)をフォルダごと同じディレクトリに置く。（英語の場合は指定しなくても動作する。）\n","- 入力ファイルは、一つ前にクレンジングした， wagahai_list.pickleを使います。\n","- 出力ファイルは、neko_cloud.txt,neko_cloud.pngを指定している。\n","\n","以下のコードは、「吾輩は猫である」　 の内容に出現する名詞のみをカウントし、WordCloudに表示する。\n","\n","#### WordCloud とは：\n","頻出語を頻度に比例する大きさで雲のように表示する。　.png形式で出力できる。"]},{"cell_type":"markdown","metadata":{"id":"UAeBFFUCxfpe"},"source":["### 1. 「吾輩は猫である」\"wagahai_list.pickle\"を読み込み，名詞である単語のみを抽出する。\n","適宜、行頭 `#print(xxxx)` のコメントアウト記号\"#\"を適宜削除し、結果を確認しながら動作確認をしてみましょう。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mA3LPQUExfpe"},"outputs":[],"source":["from janome.tokenizer import Tokenizer\n","from janome.analyzer import Analyzer\n","from janome.charfilter import *\n","from janome.tokenfilter import *\n","import pickle\n","\n","#フォントは同じディレクトリにIPAGTTC00303をコピーしておく\n","\n","#この場合の意味をなさない語を除外するためのストップワード辞書をリストにしておく\n","stop_words = ['する', 'せる', 'られる', 'あの', 'する', 'ある', 'とこ', 'なる', 'ない', 'ああ', 'れる', 'さん', 'やる', 'この', 'どう', 'そう', 'ある', 'その', 'そんな', 'いる', 'それで', 'また', 'なる']\n","\n","#pickleにしたファイルを読み込んでリストにする．\n","with open('wagahai_list.pickle', mode='rb') as f:\n","    wagahai_list = pickle.load(f)\n","#print(wagahai_list)　#wagashi_listの内容を確認する\n","\n","wagahai_text = '\\n' . join(wagahai_list) #wagahai_listの要素を改行コードで連結する．\n","\n","#print(wagahai_text) #wagashi_textの内容を確認する\n","\n","#Analyzeを用いた形態素を処理するフィルターを生成\n","token_filters = [ POSKeepFilter(['名詞']),#←形態素の種類を指定               \n","                 LowerCaseFilter(),\n","                 ExtractAttributeFilter('surface')]\n","\n","a = Analyzer(token_filters=token_filters)#Analyzerに設定したフィルターを指定する．\n","             \n","path_w = 'neko_cloud.txt' #出力ファイル名の指定\n","\n","#単語を空白で区切り出力する\n","with open(path_w, mode='w') as f:\n","    for token in a.analyze(wagahai_text): #wagahai_textをアナライザーに渡した結果を要素ひとつづつ取り出して処理\n","        line = token + ' ' #取り出した要素と空白1個を連結したものを一行とする．\n","        f.write(line) #出力ファイルに書き込む\n","\n","#生成したファイルの内容確認\n","with open(\"./neko_cloud.txt\", mode='r', encoding='utf-8') as f:\n","    s = f.read()    \n","#print(s)\n","    "]},{"cell_type":"markdown","metadata":{"id":"t9ffV3Nmxfpf"},"source":["名詞かつストップワードが除去された結果になっているか確認しましょう。　ストップワードに追加できそうな語はあるでしょうか。"]},{"cell_type":"markdown","metadata":{"id":"2RmDot0ixfpf"},"source":["### 2. 1で生成した名詞のみの単語を出力した\"neko_cloud.txt\"を使ってwordcloudモジュールを用いて、単語を可視化する。<br>\n","ライブラリ等<br>\n","- re : 正規表現を求める\n","- WordCloud : WordCloudの処理をする\n","- PIL: Python Image Library イメージを出力する\n","- numpy : ベクトルや行列などの数学的対象を処理する、機械学習やディープラーニングに便利"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ta8W5AKLxfpg"},"outputs":[],"source":["import re\n","from wordcloud import WordCloud\n","from PIL import Image\n","import numpy as np\n","\n","#名詞のみ出力したファイルを読み込む，\n","infile = 'neko_cloud.txt'\n","with open(infile, encoding='utf-8') as f:\n","       text_n= f.readlines()\n","\n","#デフォルト長方形に描画する場合\n","#WordCloudの実装\n","wordcloud = WordCloud(background_color='white',  #バックグラウンドの色\n","                      #colormap=\"summer\", #カラーマップ（色味) お好みで指定してもしなくても\n","                      font_path='./IPAGTTC00303/ipag.ttc',  #フォント指定\n","                      max_words=200,   #単語数の指定\n","                      width=800, height=400, #長方形の大きさ指定\n","                      stopwords=set(stop_words)).generate(text_n[0]) #ストップワードの指定\n","\n","#結果を画像に保存（出力ファイル名は適宜変更)\n","wordcloud.to_file(\"./neko_cloud.png\")\n","               \n","\n"]},{"cell_type":"markdown","metadata":{"id":"bAchM0Jwxfpg"},"source":["結果は， noko_cloud.png に出力されました． <br>\n","ファイルをクリックして表示させてみましょう．<br>\n","各単語の大きさは，出現頻度に比例して大きさが違っています．\n","「吾輩は猫である」の特徴がわかる結果でしたか？"]},{"cell_type":"markdown","metadata":{"id":"USX7NQ3dxfpg"},"source":["## 課題4:\n","ストップワードの追加、色あい， 大きさを変更してみましょう．<br>\n","上で出力したpngファイルと比較してみましょう．  \n","**参考：** カラーの変更　 https://karupoimou.hatenablog.com/entry/2019/05/17/153207"]},{"cell_type":"code","source":[],"metadata":{"id":"_iyRxHvb1Kds"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n6ztylRwxfph"},"source":["## 課題5:\n","前の方法では，　複合名詞がバラバラの状態になっています．複合名詞に対応するように変更してみてください。<br>\n","以下のコード中に　「#複合名詞の設定」がある行で，　カンマの直前に複合名詞の設定を挿入します。<br>\n","複合名詞を指定する設定：　CompoundNounFilter()\n","\n","複合名詞を考慮したWordCloudを出力し、 .png ファイルを開き、複合名詞を考慮した場合としない場合の比較を行ってください。  \n","<font color=\"MediumBlue\">出力するファイル名は、適宜変更して実行すること。上書きされてしまいます。</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmRTqxyLxfph"},"outputs":[],"source":["from janome.tokenizer import Tokenizer\n","from janome.analyzer import Analyzer\n","from janome.charfilter import *\n","from janome.tokenfilter import *\n","import pickle\n","\n","#フォントは同じディレクトリにIPAGTTC00303をコピーしておく\n","\n","#この場合の意味をなさない語を除外するためのストップワード辞書をリストにしておく\n","stop_words = ['する', 'せる', 'られる', 'あの', 'する', 'ある', 'とこ', 'なる', 'ない', 'ああ', 'れる', 'さん', 'やる', 'この', 'どう', 'そう', 'ある', 'その', 'そんな', 'いる', 'それで', 'また', 'なる']\n","\n","#pickleにしたファイルを読み込んでリストにする．\n","with open('wagahai_list.pickle', mode='rb') as f:\n","    wagahai_list = pickle.load(f)\n","#print(wagahai_list)　#wagashi_listの内容を確認する\n","\n","wagahai_text = '\\n' . join(wagahai_list) #wagahai_listの要素を改行コードで連結する．\n","\n","#print(wagahai_text) #wagashi_textの内容を確認する\n","\n","#Analyzeを用いたフィルター\n","token_filters = [,#←複合名詞の設定\n","                 POSKeepFilter(['名詞']),\n","                 LowerCaseFilter(),\n","                 ExtractAttributeFilter('surface')]\n","\n","a = Analyzer(token_filters=token_filters)\n","             \n","path_w = 'neko_cloud.txt' #出力ファイル名の指定\n","\n","#単語を空白で区切り出力する\n","with open(path_w, mode='w') as f:\n","    for token in a.analyze(wagahai_text): #wagahai_textをアナライザーに渡した結果を要素ひとつづつ取り出して処理\n","        line = token + ' ' #取り出した要素と空白1個を連結したものを一行とする．\n","        f.write(line) #出力ファイルに書き込む\n","\n","#生成したファイルの内容確認\n","with open(\"./neko_cloud.txt\", mode='r', encoding='utf-8') as f:\n","    s = f.read()    \n","print(s)"]},{"cell_type":"markdown","metadata":{"id":"DeXERy6Sxfpi"},"source":["## 課題6:\n","同様に前の課題で各自青空文庫からダウンロードして前処理をしたファイル(.pckle)を用いて、複合名詞を考慮した場合のWordCloudを生成してみてください。\n","小説の特徴を表している単語が大きく見えているでしょうか？"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sKYxIGMRxfpi"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}