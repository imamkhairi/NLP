{"cells":[{"cell_type":"markdown","metadata":{"id":"J7M4XLq57M9y"},"source":["# 文章の類似度\n","Doc2Vecは、任意の長さの文書をベクトル化する技術です。  \n","この技術を使って、文書やテキストの分散表現を獲得し、類似度を計算します。"]},{"cell_type":"markdown","metadata":{"id":"qulqO0CM7M9z"},"source":["## データの読み込み\n","以前に作成したデータの読み込みを行います。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8tnVePm37M9z"},"outputs":[],"source":["import pickle\n","\n","with open('wagahai_words.pickle', mode='rb') as f:\n","    wagahai_words = pickle.load(f)\n","\n","print(wagahai_words)"]},{"cell_type":"markdown","metadata":{"id":"YXyeMw-77M90"},"source":["## doc2vecによる学習\n","doc2vecを使って学習を行い、モデルを作成します。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GEizS4nv7M90"},"outputs":[],"source":["from gensim.models.doc2vec import Doc2Vec\n","from gensim.models.doc2vec import TaggedDocument\n","\n","tagged_documents = []\n","for i, sentence in enumerate(wagahai_words):\n","    tagged_documents.append(TaggedDocument(sentence, [i]))  # TaggedDocument型のオブジェクトをリストに格納\n","\n","# size：分散表現の次元数\n","# window：対象単語を中心とした前後の単語数\n","# min_count：学習に使う単語の最低出現回数\n","# epochs:epochs数\n","# dm：学習モデル=DBOW（デフォルトはdm=1で、学習モデルはDM）\n","model = Doc2Vec(documents=tagged_documents,\n","                vector_size=100,\n","                min_count=5,\n","                window=5,\n","                epochs=20,\n","                dm=0)"]},{"cell_type":"markdown","metadata":{"id":"s_Aem2kK7M90"},"source":["## 文章のベクトル\n","最初の文章（0番目）のベクトルを表示します。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YAKoXR2G7M91"},"outputs":[],"source":["print(wagahai_words[0])  # 最初の文章を表示\n","print(model.docvecs[0])  # 最初の文章のベクトル"]},{"cell_type":"markdown","metadata":{"id":"op5vfaNU7M91"},"source":["リストの0番目の文 \"吾輩は猫である” という文と他の100文との分散表現の値が，0.05046845, 0.011611, .., 0.07975786の100次元ベクトル(vector_size=100)になっていることが確認できますね。"]},{"cell_type":"markdown","metadata":{"id":"UGR_bAXm7M91"},"source":["## 文章の類似度\n","最初の文章（0番目）と最も類似度の高い文章のIDと類似度を表示します。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8qGzAWF_7M91"},"outputs":[],"source":["print(model.docvecs.most_similar(0))"]},{"cell_type":"markdown","metadata":{"id":"NTDC5uN47M91"},"source":["(3782, 0.9988241195678711)　 は リストの3782番目の文章との類似度が0.9988241195678711という意味になります。 <br>\n","それでは、類似度が高かった文章を表示してみましょう。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T3rmtM7E7M91"},"outputs":[],"source":["for p in model.docvecs.most_similar(0):\n","    print(wagahai_words[p[0]])"]},{"cell_type":"markdown","metadata":{"id":"Ri4_UtXl7M92"},"source":["類似度の高い文章が表示されました。  \n","\"吾輩は猫である\"と似ている文章が出力されていましたか？ <br>\n","文頭が\"吾輩\"で文末が\"である\"のような文章が多くみられますね。  \n","doc2vecにより、文書全体の類似度を計算することも可能です。各小説ごとの類似度を求められることになり，同じ作者の小説の類似度を求めてみるのも面白そうです。"]},{"cell_type":"markdown","metadata":{"id":"MEEyDR2q7M92"},"source":["### 課題4:\n","wagahai_wordsの中の適当な文章（リスト番号で指定）と、類似度の高い文章を表示してみましょう。　<br>\n","考察：出力された文章と似ている文章が出力されましたか？"]},{"cell_type":"code","source":[],"metadata":{"id":"NbxUzyz2-7nx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UzIs7RNl7M92"},"source":["### 課題5:\n","\n","前週までに各自青空文庫からダウンロードし、前処理をしpickle化したpickleファイルを用いて、10番目の文章の文章とベクトル表現を示し、その文と最も類似度が高い文章の(ID,類似度)と、文章（リスト形式でよい）を表示してみましょう。<br>\n","\n","考察：出力結果は10番目の文章と似ていましたか？"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ui_hq0fD7M92"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}