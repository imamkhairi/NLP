{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 単語の分散表現\n",
    "word2vecによる分散表現について学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コーパスの前処理\n",
    "前のセクションと同様に、コーパスに前処理を行います。\n",
    "\n",
    "**Tips:** コードセルに行番号を表示させたいときは、「esc」+「l」（エスケープ、小文字のエル）または、Jupyterのメニュー「表示」→「行番号トグル」選択する。<br>\n",
    "同じディレクトリに 「吾輩は猫である」のテキストファイル \"wagahaiwa_nekodearu.txt\"があることを確認してから始めましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "with open(\"wagahaiwa_nekodearu.txt\", mode=\"r\", encoding=\"utf-8\") as f:  # ファイルの読み込み\n",
    "    wagahai_original = f.read()\n",
    "\n",
    "wagahai = re.sub(\"《[^》]+》\", \"\", wagahai_original) # ルビの削除\n",
    "wagahai = re.sub(\"［[^］]+］\", \"\", wagahai) # 読みの注意の削除\n",
    "wagahai = re.sub(\"[｜ 　「」\\n]\", \"\", wagahai) # | と全角半角スペース、「」と改行の削除\n",
    "\n",
    "seperator = \"。\"  # 。をセパレータに指定\n",
    "wagahai_list = wagahai.split(seperator)  # セパレーターを使って文章をリストに分割する\n",
    "wagahai_list.pop() # 最後の要素は空の文字列になるので、削除\n",
    "wagahai_list = [x+seperator for x in wagahai_list]  # 文章の最後に。を追加\n",
    "        \n",
    "t = Tokenizer()\n",
    "\n",
    "wagahai_words = []\n",
    "for sentence in wagahai_list:\n",
    "    wagahai_words.append(t.tokenize(sentence, wakati=True))   # 文章ごとに単語に分割し、リストに格納\n",
    "    \n",
    "with open('wagahai_words.pickle', mode='wb') as f:  # pickleに保存\n",
    "    pickle.dump(wagahai_words, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存できていることを確認します。<br>\n",
    "同じディレクトリにwagahai_words.pickleが生成されているか確認しましょう。前処理の結果（リスト形式）を塩漬けにしました。<br>\n",
    "それでは、中身を確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wagahai_words.pickle', mode='rb') as f:\n",
    "    wagahai_words = pickle.load(f)\n",
    "\n",
    "print(wagahai_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "１文ごとに、形態素のかたまりのリスト表現になっています。全体もリスト形式になっていますね。\\[[1文の形態素リスト], [1文の形態素リスト], [1文の形態素リスト], ...\\]<br>\n",
    "それでは、この形式のデータを用いてword2vecを使っていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vecを用いた学習\n",
    "今回はword2vecのためにライブラリgensimを使います。  \n",
    "gensimは、様々なトピックモデルを実装したPythonライブラリです。  \n",
    "トピックモデルとは、潜在的なトピックから文章が確率的に生成されると仮定したモデルです。\n",
    "\n",
    "gensimについて、詳細は以下のリンクを参考にどうぞ。  \n",
    "https://radimrehurek.com/gensim/\n",
    "\n",
    "以下では、word2vecを用いてコーパスの学習を行い、学習済みのモデル（分散表現を生成）を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# size : 中間層のニューロン数\n",
    "# min_count : この値以下の出現回数の単語を無視\n",
    "# window : 対象単語を中心とした前後の単語数\n",
    "# iter : epochs数\n",
    "# sg : skip-gramを使うかどうか 0:CBOW 1:skip-gram\n",
    "model = word2vec.Word2Vec(wagahai_words,\n",
    "                          size=100,\n",
    "                          min_count=5,\n",
    "                          window=5,\n",
    "                          iter=20,\n",
    "                          sg = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modelはwagahai_words.pickleの各単語（ここでは、形態素）をsize~sgに設定したパラメータでCBOWで学習したモデルです。\n",
    "\n",
    "学習したモデルmodel（分散表現）の確認をします。<br>\n",
    "それでは、分散表現を見ていきましょう。  \n",
    "重みを表す行列（分散表現）の形状と、行列そのものを表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.vectors.shape)  # 分散表現の形状\n",
    "print(model.wv.vectors)  # 分散表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初めにプリントされている(3309,100)は、単語数は3309個、中間層のニューロン数は100次元（1の説明資料p3にある「200程度」の次元が100ということになる）を意味します。<br>\n",
    "[[単語1の１００次元のベクトル],[単語2の１００次元のベクトル],[単語3の１００次元のベクトル], ..., [単語3309の１００次元のベクトル]]のベクトル表現になっています。<br>\n",
    "\n",
    "語彙の数、および語彙の最初の10語を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.wv.index2word))  # 語彙の数\n",
    "print(model.wv.index2word[:10])  # 最初の10単語"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "語彙における、最初の単語の単語ベクトルを2通りの方法で表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.vectors[0])  # 最初のベクトル\n",
    "print(model.wv.__getitem__(\"の\"))  # 最初の単語「の」のベクトル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "どちらも **'の'** のベクトル\\[100次元\\]\\[100次元\\]です。両者ともに同じベクトルですね。<br>\n",
    "wagahaiha_nekodearu.txtに出現する単語の分散表現を生成できました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 課題1:\n",
    "1. 単語「猫」を単語ベクトルで表してみましょう。  \n",
    "2. 次に、CBOWをskip-gramに変更して、試して結果を比較してみましょう。同様に「猫」の結果で比較します。<br>\n",
    "このとき、\"model\" の名前を変更して学習済みモデルを生成してください。<br>\n",
    "\n",
    "考察： CBOWとskip-gramで生成した学習済みモデル（分散表現）に違いはみられましたか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
