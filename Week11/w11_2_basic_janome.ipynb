{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAp1VGAVic54"
   },
   "source": [
    "## 1月16日（月）　自然言語処理　実習1-2\n",
    "\n",
    "1. 簡単なpythonの基礎\n",
    "2. 形態素解析器 janome を使ってみる．\n",
    "\n",
    "形態素解析器Janomeはpythonを利用する時にインストールが手軽にできること，また，形態素解析がどんなものか試すには魅力的である． <br> \n",
    "繰り返し長文を解析したり大量のテキストを解析したりする必要がある場合はMeCab, Juman++, sudachi， GINZAなどをおすすめする．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZ467U-eic57"
   },
   "source": [
    "## 2. Janomeを使ってみる．\n",
    "\n",
    "[基本]<br>\n",
    "1. Janomeがもつ機能Tokenizerをインポートする．<br> \n",
    "2. Tokenizerオブジェクトのインスタンスを生成.<br>\n",
    "3. tokenize()メソッドに対象の文字列を渡す.<br>\n",
    "\n",
    "tokenize()メソッドはjanome.tokenizer.Tokenオブジェクトを要素とするリストを返します．<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eou1uLSyic57"
   },
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "s = 'すもももももももものうち'\n",
    "\n",
    "print(type(t.tokenize(s)))# 返す値のタイプを確認\n",
    "print(type(t.tokenize(s)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1l88RjGNic58"
   },
   "source": [
    "'すもももももももものうち'　を形態素解析する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fFGDPOuic59"
   },
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "s = 'すもももももももものうち'\n",
    "\n",
    "for token in t.tokenize(s):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqrBHvdJic59"
   },
   "source": [
    "***\n",
    "<font color=\"SteelBlue\">\n",
    " Tokenオブジェクトの属性から情報を取得する.<br>\n",
    " 品詞情報は ***品詞,品詞細分類1,品詞細分類2,品詞細分類3*** の4つが表示される． ただし，細分類が定義されていない場合は __*__ と表示される．\n",
    "</font>\n",
    "\n",
    "|token.メソッド|機能|\n",
    "|----------------------|-------------------------------------------------------------------|\n",
    "|surface|文字列中で使われているそのままの形|\n",
    "|part_of_speech|品詞|\n",
    "|infl_type|活用型|\n",
    "|infl_form|活用形|\n",
    "|base_form|基本形，見出し語|\n",
    "|reading|読み|\n",
    "|phonetic|発音|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a-YS2abic59"
   },
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "token = t.tokenize('走れ')[0]\n",
    "print(type(token))#tokenの内容のタイプを表示\n",
    "print('表層= ',token.surface)\n",
    "print('品詞,品詞細分類1,品詞細分類2,品詞細分類3= ',token.part_of_speech)\n",
    "print('品詞[0,1,2,3]= ',token.part_of_speech.split(','))#品詞情報4つを',' でわけ，それぞれ表示する\n",
    "print('品詞[0]= ',token.part_of_speech.split(',')[0])#品詞情報を' ,'でわけた，　0番目を表示する．\n",
    "print('活用型= ',token.infl_type)\n",
    "print('活用形= ',token.infl_form)\n",
    "print('基本形，見出語= ',token.base_form)\n",
    "print('読み= ',token.reading)\n",
    "print('発音= ',token.phonetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLffM6M9ic5-"
   },
   "source": [
    "***\n",
    "<font color=\"SteelBlue\">\n",
    "分かち書きをしてみる．（単語ごとに分割する）<br>\n",
    "英語は単語ごとにスペースで区切られているのでスペースでsplitすればよい．<br>\n",
    "\n",
    " **例文：「走れと言われたので走ると言った」** \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7_rPucaic5-"
   },
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer()#Tokenizerオブジェクトのインスタンスを生成\n",
    "\n",
    "s = '走れと言われたので走ると言った'\n",
    "\n",
    "for token in t.tokenize(s):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpkqT_Oric5-"
   },
   "source": [
    "***\n",
    "### <font color=\"SteelBlue\">引数wakati</font>\n",
    "tokenize()メソッドの引数wakatiをTrueとするとTokenオブジェクトのリストではなく，　表層形の文字列strのリストを返す。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_gT_lxXic5_"
   },
   "outputs": [],
   "source": [
    "print(t.tokenize(s, wakati=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5keh7EVic5_"
   },
   "source": [
    "***\n",
    "### <font color=\"SteelBlue\"> リスト内包表記</font>\n",
    "\n",
    "リスト内包表記を使うとTokenオブジェクトから所望の属性を取り出してリスト化できる。<br>\n",
    "元の文字列をそのまま分かち書きしたい場合はsurface属性を使う。<br>\n",
    "    引数wakatiをTrueとした場合の結果と同じ記法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfWpXveyic5_"
   },
   "outputs": [],
   "source": [
    "print([token.surface for token in t.tokenize(s)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2rNORL0ic5_"
   },
   "source": [
    "base_formやpart_of_speechで基本形や品詞のリストを取得することも可能である．<br>\n",
    "上のsurfaceの部分を変更して試そう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHjQ_TD9ic5_"
   },
   "outputs": [],
   "source": [
    "print([token.base_form for token in t.tokenize(s)])\n",
    "print([token.part_of_speech.split(',')[0] for token in t.tokenize(s)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0_8ctyxic6A"
   },
   "source": [
    "リスト内包表記で **if** を使うと特定の品詞のみをリストアップできる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWfSgxeYic6A"
   },
   "outputs": [],
   "source": [
    "print([token.surface for token in t.tokenize(s)\n",
    "       if token.part_of_speech.startswith('動詞')])#品詞が動詞の場合\n",
    "\n",
    "print([token.surface for token in t.tokenize(s)\n",
    "       if token.part_of_speech.startswith('動詞,自立')])#品詞が動詞であり品詞細分類が自立である場合\n",
    "\n",
    "print([token.surface for token in t.tokenize(s)\n",
    "       if token.part_of_speech.split(',')[0] in ['動詞', '助動詞']])#品詞の[0](つまり品詞)が動詞または助動詞の場合\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeBM2k6kic6A"
   },
   "source": [
    "##課題1-1：\n",
    "　文l = 'メロスは寒い冬に稚内から東京まで自転車で走った'を定義し，\n",
    "(l 小文字エル)\n",
    "1. 文l において，品詞が名詞である単語を表示してみる．\n",
    "2. 文l において，品詞が助詞，格助詞である単語を表示してみる．\n",
    "3. 文l において，品詞が名詞または形容詞である単語を表示してみる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zG1FXIyic6A"
   },
   "outputs": [],
   "source": [
    "l  = 'メロスは寒い冬に稚内から東京まで自転車で走った'\n",
    "\n",
    "print([token.surface for token in t.tokenize(l)\n",
    "       if token.part_of_speech.startswith('名詞')])\n",
    "\n",
    "print([token.surface for token in t.tokenize(l)\n",
    "       if token.part_of_speech.startswith('助詞,格助詞')])\n",
    "\n",
    "print([token.surface for token in t.tokenize(l)\n",
    "       if token.part_of_speech.split(',')[0] in ['名詞', '形容詞']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY4GpBvjic6A"
   },
   "source": [
    "***\n",
    "### <font color=\"SteelBlue\"> 単語の出現回数をカウントする</font>\n",
    "\n",
    "方法<br>\n",
    "1. 分かち書きをしてそれぞれの単語をカウントする．\n",
    "2. Counter オブジェクトの most_common()メソッドを使う．\n",
    "3. TokenCountFilter()を使う.\n",
    "\n",
    "Python標準ライブラリcollectionsのCounterクラスを使う．<br>\n",
    "方法1："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWgqgSLyic6B"
   },
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "import collections\n",
    "\n",
    "t = Tokenizer()\n",
    "s= '赤巻紙青巻紙黄巻紙'\n",
    "\n",
    "for token in t.tokenize(s):\n",
    "    print(token)\n",
    "\n",
    "c = collections.Counter(t.tokenize(s, wakati=True))\n",
    "\n",
    "print(type(c))\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2o15DiDic6B"
   },
   "source": [
    "単語を指定すると出現回数を取得できる.　存在しない単語は0を返す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SiT0sauqic6B"
   },
   "outputs": [],
   "source": [
    "print(c['赤'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2Pmv8Feic6B"
   },
   "source": [
    "方法2：　Counter オブジェクトの most_common()メソッドを使う．<br>\n",
    "出現回数の降順で　タプル　(単語, 出現回数)　が要素のリストを返す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tw0BJs5oic6B"
   },
   "outputs": [],
   "source": [
    "mc = c.most_common()\n",
    "print(mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98WGRY0wic6C"
   },
   "source": [
    "方法3：　janomeのフレームワークを使う.<br>\n",
    "- CharFilter（対象文字列の正規化などの前処理）\n",
    " - UnicodeNormalizeCharFilter() Unicodeをunicodedata.normalize()で正規化\n",
    " - RegexReplaceCharFilter() 正規表現で置換\n",
    "- TokenFilter（トークンのフィルタリングなどの後処理）\n",
    " - CompoundNounFilter() 連続する名詞の複合名詞化\n",
    " - ExtractAttributeFilter()  抽出する属性（‘surface’や‘part_of_speech’など）を指定\n",
    " - LowerCaseFilter() / UpperCaseFilter() アルファベットを小文字 / 大文字に変換\n",
    " - POSKeepFilter() / POSStopFilter() 結果に含む / 結果から除外する品詞をリストで指定\n",
    " - TokenCountFilter() 出現回数をカウント\n",
    "\n",
    "(単語, 出現回数)のタプルをジェネレーターで返す.<br>\n",
    "基本的な使い方：　analyzerとcharfilter, tokenfilterをインポートする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThvRuainic6C"
   },
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.charfilter import *\n",
    "from janome.tokenfilter import *\n",
    "\n",
    "#t = Tokenizer()\n",
    "\n",
    "s= '花子は赤巻紙青巻紙黄巻紙と言った'\n",
    "\n",
    "a = Analyzer(token_filters=[POSKeepFilter(['名詞']), TokenCountFilter()]) #POSKeepFilter()で名詞のみをカウントする\n",
    "\n",
    "g_count = a.analyze(s)\n",
    "print(type(g_count)) #g_countのタイプを表示\n",
    "\n",
    "i=0\n",
    "for i in g_count:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nPn9lVZic6C"
   },
   "source": [
    "\\<div\\>PythonとＰＹＴＨＯＮとパイソンとﾊﾟｲｿﾝ\\<\\/div\\>　の全角を半角に変換し、正規表現によりHTMLタグを消去（空文字列で置換）し，　さらに名詞のみを抽出してアルファベットを小文字化、'surface'属性（文中のそのままの表現)のみを抽出する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9FJ12JVic6C"
   },
   "outputs": [],
   "source": [
    "from janome.analyzer import Analyzer\n",
    "from janome.charfilter import *\n",
    "from janome.tokenfilter import *\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "s = '<div>PythonとＰＹＴＨＯＮとパイソンとﾊﾟｲｿﾝ</div>'\n",
    "\n",
    "for token in t.tokenize(s):\n",
    "    print(token) #形態素解析の結果を表示\n",
    "    \n",
    "#CharFilterおよびTokenFilterをリストで指定して\n",
    "char_filters = [UnicodeNormalizeCharFilter(), \n",
    "                RegexReplaceCharFilter('<.*?>', '')]\n",
    "\n",
    "token_filters = [POSKeepFilter(['名詞']),\n",
    "                 LowerCaseFilter(),\n",
    "                 ExtractAttributeFilter('surface')]\n",
    "\n",
    "#Analyzerオブジェクトを生成\n",
    "a = Analyzer(char_filters=char_filters, token_filters=token_filters)\n",
    "\n",
    "for token in a.analyze(s):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ue5fzfzqic6C"
   },
   "source": [
    "CompoundNounFilter()による複合名詞の抽出<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsfwXYlOic6C"
   },
   "outputs": [],
   "source": [
    "s = '釧路工業高等専門学校の創造工学科の情報工学分野'\n",
    "\n",
    "for token in t.tokenize(s):\n",
    "    print(token)\n",
    "\n",
    "print('================\\n')\n",
    "a = Analyzer(token_filters=[CompoundNounFilter()])\n",
    "\n",
    "for token in a.analyze(s):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GiYWxoiic6C"
   },
   "source": [
    "TokenCountFilter()で単語の出現回数をカウントする.<br>\n",
    "(単語, 出現回数)のタプルを返す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G85iJlIZic6C"
   },
   "outputs": [],
   "source": [
    "s = '赤巻紙青巻紙黄巻紙'\n",
    "a = Analyzer(token_filters=[POSKeepFilter(['名詞']), TokenCountFilter()])\n",
    "\n",
    "g_count = a.analyze(s)\n",
    "print(type(g_count))\n",
    "\n",
    "print('================\\n')\n",
    "\n",
    "for i in g_count:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCep88Y0ic6D"
   },
   "source": [
    "リスト化したい場合はlist()を使う."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBwe7TGaic6D"
   },
   "outputs": [],
   "source": [
    "l_count = list(a.analyze(s))\n",
    "print(type(l_count))\n",
    "\n",
    "print('================\\n')\n",
    "\n",
    "print(l_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbAKhhl6ic6D"
   },
   "source": [
    "dict()で辞書（dict型オブジェクト）に変換することも可能．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wG052sS-ic6D"
   },
   "outputs": [],
   "source": [
    "d_count = dict(a.analyze(s))\n",
    "print(type(d_count))\n",
    "\n",
    "print('================\\n')\n",
    "\n",
    "print(d_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iOKEe4xic6D"
   },
   "source": [
    "辞書のキーを指定して単語の出現回数を取得可能．<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06TW0qVdic6D"
   },
   "outputs": [],
   "source": [
    "print(d_count['巻紙'])\n",
    "print('================\\n')\n",
    "\n",
    "print(d_count['巻髪'])\n",
    "print('================\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Dfv95ERic6D"
   },
   "source": [
    ".getを以下のように使うことで， 単語がない場合，エラーとならず，0を返すことができる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJrgCUyvic6D"
   },
   "outputs": [],
   "source": [
    "print(d_count.get('巻髪', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3mAW7E5ic6D"
   },
   "source": [
    "TokenCountFilter()の引数attrでカウントする属性を指定し，　動詞の基本形をカウントしてみる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNELr-_Ric6E"
   },
   "outputs": [],
   "source": [
    "a = Analyzer(token_filters=[TokenCountFilter()])\n",
    "\n",
    "s = '走れと言われたので走ると言った'\n",
    "\n",
    "print('文中で使っているままの単語を表示')\n",
    "print(list(a.analyze(s)))\n",
    "\n",
    "a = Analyzer(token_filters=[TokenCountFilter(att='base_form')]) #base_formで単語の原型を指定\n",
    "\n",
    "print('単語の原型を表示')\n",
    "print(list(a.analyze(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23xF2Z22ic6E"
   },
   "source": [
    "'part_of_speech'を指定した場合は細分類を含んだ文字列となる．<br>\n",
    "品詞だけでカウントしたい場合は上述の内包表記を用いた方法を使う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0AJCoKkic6E"
   },
   "outputs": [],
   "source": [
    "a = Analyzer(token_filters=[TokenCountFilter(att='part_of_speech')])\n",
    "\n",
    "print(list(a.analyze(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlBbra1Gic6E"
   },
   "source": [
    "POSKeepFilter(), POSStopFilter()で品詞を限定したり除外が可能，　ひとつだけ指定する場合もリストを使う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYmxGVtcic6E"
   },
   "outputs": [],
   "source": [
    "s = '太郎は花子に花を贈った'\n",
    "\n",
    "a = Analyzer(token_filters=[POSKeepFilter('助動詞')]) #品詞を指定する単語をそのまま指定すると正しく抽出できない\n",
    "\n",
    "print('正しく抽出されない場合(助動詞に含まれる助詞，動詞もとれてしまう)')\n",
    "for token in a.analyze(s):\n",
    "    print(token)\n",
    "\n",
    "a = Analyzer(token_filters=[POSKeepFilter(['助動詞'])])#リストで指定すると改善される\n",
    "\n",
    "print('=========\\n改善した結果')\n",
    "for token in a.analyze(s):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQnA0THvic6E"
   },
   "source": [
    "POSStopFilter で品詞を除外する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xa9TjRg5ic6E"
   },
   "outputs": [],
   "source": [
    "s = '太郎は花子に花を贈った'\n",
    "\n",
    "a = Analyzer(token_filters=[POSStopFilter(['助詞'])]) #品詞をしていする単語はリストで指定\n",
    "\n",
    "for token in a.analyze(s):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDRtpHO1ic6E"
   },
   "source": [
    "***\n",
    "### <font color=\"SteelBlue\"> ゲンシゴホンヤクを作ってみよう</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0jNgGaUic6E"
   },
   "source": [
    "1文を入力するとゲンシ語に翻訳します．　ゲンシ語に翻訳する規則を次のように設定します．<br>\n",
    "1. すべてをカタカナに変換\n",
    "2. 「助詞」を取り除く\n",
    "\n",
    "「助詞」の場合には出力がなく，「助詞以外」の場合はヨミガナを出力するようにするとよさそうです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8D-gQc4Iic6F"
   },
   "outputs": [],
   "source": [
    "#Janomeのロード\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "#Toknizerインスタンスの生成\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "#形態素解析の実施. 形態素解析の結果がtokensに入る．\n",
    "tokens = tokenizer.tokenize(\"これでみんなで原始人。肉を食べよう！\")\n",
    "\n",
    "#tokenが助詞の場合は空文字列，それ以外はヨミガナ(.reading)を返す関数\n",
    "def token2gensigo(input_token):\n",
    "    if input_token.part_of_speech.split(',')[0] == \"適切な品詞を書く\":\n",
    "        return \"\"\n",
    "    else:\n",
    "        return input_token.reading\n",
    "    \n",
    "#各tokenの変換結果を\"  \" (半角スペース)でつなげる. 結果を入れる変数を用意する．\n",
    "result_str = \"\"\n",
    "#全てのtokenに，上でdef（定義）したtoken2gensigo関数を実行\n",
    "for token in tokens:\n",
    "    result_str += token2gensigo(token) + \" \"\n",
    "    \n",
    "print(result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7ZuTqK9ic6F"
   },
   "source": [
    "##課題1-2： ##  \n",
    "上記のプログラムを変更し，助詞を取り除きゲンシ語翻訳を完成させよう．　オリジナルな3文に対して実行例を示してください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bkIl1Mkic6F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvP6nD5ijR78"
   },
   "source": [
    "##課題1-3:##  \n",
    "ゲンシ人がいっぱい襲来してきたときにさばきやすいように改良してみよう．  \n",
    "「ヨミガナを返す関数にtokenを渡しヨミガナを取得」, 「ゲンシ語の結果を返すように書く」，「適切な品詞を書く」を適切なコードに書き換えて日本語（一文）をゲンシ語（一文）に変換する関数nihongo2gensigo()と単語のヨミガナを返す関数token2gensigo()を定義して動作確認をしてください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMbsar5Pk3z3"
   },
   "outputs": [],
   "source": [
    "# Janomeのロード\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# Tokenizerインスタンスの生成 \n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# 日本語をゲンシゴにする関数\n",
    "def nihongo2gensigo(input_str):\n",
    "  # 形態素解析の実施\n",
    "  tokens = tokenizer.tokenize(input_str)\n",
    "  # 各token の変換結果を\" \"（半角スペース) でつなげる\n",
    "  result_str = \"\"\n",
    "  for token in tokens:\n",
    "    result_str += ヨミガナを返す関数にtokenを渡しヨミガナを取得 + \" \"\n",
    "  return ゲンシ語の結果を返すように書く\n",
    "\n",
    "# tokenが助詞の場合は空文字列、それ以外はヨミガナを返す関数\n",
    "def token2gensigo(input_token):\n",
    "  if input_token.part_of_speech.split(',')[0] == \"適切な品詞を書く\":\n",
    "    return \"\"\n",
    "  else:\n",
    "    return input_token.reading\n",
    "\n",
    "print(nihongo2gensigo(\"人民の人民による人民のための政治\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJNNr2UGloVf"
   },
   "source": [
    "上のコードが完成したら，課題1-2で実行例に使ったオリジナル文を用いて3人のゲンシジンの襲来をnihongo2gensigo()を用いて連続技で実行してみよう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFNjPmtFlm_j"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
